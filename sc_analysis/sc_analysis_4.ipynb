{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 降维\n",
    "\n",
    "## 变分自编码(VAE)\n",
    "Variational AutoEncoedr是AutoEncoder的一种，融合贝叶斯方法和深度学习的优势，拥有优雅的数学基础和简单易懂的架构以及令人满意的性能，使得它比一般的生成模型具有更广泛的意义\n",
    "\n",
    "`生成器`\n",
    "* 其目标是要得到p(z|x)的分布，即给定输入数据x的分布，得到潜在变量Z的分布\n",
    "* 为了求解真实的后验p(z|x)的概率分布，VAE引入了一个识别模型q(z|x)去近似p(z|x)，衡量这两个分布的差异使用相对熵(KL散度)，VAE的目的就是让这个相对熵越小\n",
    "$$\n",
    "-KL(q(z|x) || p(z)) = \\frac{1}{2} (1 + log(\\sigma_i^2) - \\mu_j^2 - \\sigma_j^2)\n",
    "$$\n",
    "代码实现\n",
    "\n",
    "```python\n",
    "# p(Z|X)的均值和方差\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    " # KL散度\n",
    "kl_loss = - 0.5 * K.sum(\n",
    "    1 + z_log_var - K.square(z_mean) - K.exp(z_log_var),\n",
    "    axis=-1)\n",
    "```\n",
    "\n",
    "`解码器`，输入z然后输出一个x'，目的是让x'和x的分布尽量一致，当两者完全一样时，中间的潜在变量z可以看作是x的一种压缩状态，包含了x的全部特征\n",
    "```python\n",
    "# 解码器\n",
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "# xent_loss是重构loss，\n",
    "xent_loss = K.sum(K.binary_crossentropy(x, x_decoded_mean), axis=-1)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 57\u001b[0m\n\u001b[1;32m     51\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# =======================================\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#  构建VAE模型\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# =======================================\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m sc_vae, vae_loss, encoder \u001b[38;5;241m=\u001b[39m \u001b[43mget_vae_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43moriginal_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m sc_vae\u001b[38;5;241m.\u001b[39madd_loss(vae_loss)\n\u001b[1;32m     61\u001b[0m sc_vae\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 37\u001b[0m, in \u001b[0;36mget_vae_model\u001b[0;34m(original_dim, intermediate_dim, latent_dim)\u001b[0m\n\u001b[1;32m     33\u001b[0m sc_vae \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mModel(x, x_decoded_mean)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Loss\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# xent_loss是重构loss，kl_loss是KL loss\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m xent_loss \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39msum(\u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_crossentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_decoded_mean\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m K\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m z_log_var \u001b[38;5;241m-\u001b[39m K\u001b[38;5;241m.\u001b[39msquare(z_mean) \u001b[38;5;241m-\u001b[39m K\u001b[38;5;241m.\u001b[39mexp(z_log_var), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m vae_loss \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mmean(xent_loss \u001b[38;5;241m+\u001b[39m kl_loss)\n",
      "File \u001b[0;32m~/anaconda3/envs/rna_seq/lib/python3.11/site-packages/keras/src/legacy/backend.py:277\u001b[0m, in \u001b[0;36mbinary_crossentropy\u001b[0;34m(target, output, from_logits)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras._legacy.backend.binary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbinary_crossentropy\u001b[39m(target, output, from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"DEPRECATED.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m     output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(output)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m from_logits:\n",
      "File \u001b[0;32m~/anaconda3/envs/rna_seq/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/rna_seq/lib/python3.11/site-packages/keras/src/backend/common/keras_tensor.py:138\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, backend as K\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# 采样函数，应用重参数技巧\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# 定义 VAE 模型\n",
    "def get_vae_model(original_dim, intermediate_dim, latent_dim):\n",
    "    \n",
    "    # 编码器\n",
    "    x = layers.Input(shape=(original_dim,))\n",
    "    h = layers.Dense(intermediate_dim, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim)(h)\n",
    "    z_log_var = layers.Dense(latent_dim)(h)\n",
    "\n",
    "    # 采样层\n",
    "    z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # 解码器\n",
    "    decoder_h = layers.Dense(intermediate_dim, activation='relu')\n",
    "    decoder_mean = layers.Dense(original_dim, activation='sigmoid')\n",
    "    h_decoded = decoder_h(z)\n",
    "    x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "    # 建立VAE模型\n",
    "    sc_vae = models.Model(x, x_decoded_mean)\n",
    "\n",
    "    # Loss\n",
    "    # xent_loss是重构loss，kl_loss是KL loss\n",
    "    xent_loss = K.sum(K.binary_crossentropy(x, x_decoded_mean), axis=-1)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    vae_loss = K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    # Encoder模型\n",
    "    encoder = models.Model(x, z_mean)\n",
    "\n",
    "    return sc_vae, vae_loss, encoder\n",
    "\n",
    "# 参数\n",
    "batch_size = 100\n",
    "original_dim = 1838\n",
    "latent_dim = 64\n",
    "intermediate_dim = 256\n",
    "epochs = 30\n",
    "\n",
    "\n",
    "# =======================================\n",
    "#  构建VAE模型\n",
    "# =======================================\n",
    "sc_vae, vae_loss, encoder = get_vae_model(\n",
    "    original_dim, intermediate_dim, latent_dim\n",
    ")\n",
    "sc_vae.add_loss(vae_loss)\n",
    "sc_vae.compile(optimizer='rmsprop')\n",
    "sc_vae.summary()\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 数据加载\n",
    "# =======================================\n",
    "pbmc_data = sc.datasets.pbmc3k_processed()\n",
    "\n",
    "# 读取记录\n",
    "x_data = pbmc_data.X\n",
    "y_test_ = pbmc_data.obs['louvain'].values\n",
    "\n",
    "# 归一化\n",
    "x_train = preprocessing.MinMaxScaler().fit_transform(x_data)\n",
    "\n",
    "# 模型训练\n",
    "sc_vae.fit(x_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_train, None))\n",
    "\n",
    "\n",
    "# 隐含向量\n",
    "\n",
    "\n",
    "x_test_encoded = encoder.predict(x_train, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 可视化\n",
    "# =======================================\n",
    "features = np.array(x_test_encoded).astype('float32')\n",
    "adata = sc.AnnData(features)\n",
    "adata.obs['label'] = y_test_\n",
    "# 计算邻域图\n",
    "\n",
    "\n",
    "sc.pp.neighbors(adata, n_neighbors=10, use_rep='X')\n",
    "\n",
    "# 聚类\n",
    "sc.tl.louvain(adata)\n",
    "sc.tl.umap(adata)\n",
    "sc.pl.umap(adata, color=['louvain', 'label'], save='_pbmc3k_vae_louvain.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 变分自编码+归一化\n",
    "### 批归一化BN\n",
    "batch normalization\n",
    "* BN通过规范化与线性变换使得每一层网络的输入数据的均值和方差都在一定范围内，使得后一层网络不必不断去适应地层网络中输入的变化，从而实现了网络中层与层之间的解耦，允许每一层进行独立学习，有利于提高整个神经网络学习效率\n",
    "$$\n",
    "\\mu_B = \\frac{1}{m} \\Sigma_{i=1}^m x_i\\\\\n",
    "\\sigma_B^2 = \\frac{1}{2} \\Sigma_{i=1}^m (x_i - \\mu_B)^2\\\\\n",
    "\\hat{x_i} = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\\\\\n",
    "y_i = \\gamma \\hat{x_i} + \\beta = BN_{\\gamma, \\beta}(x_i)\n",
    "$$\n",
    "\n",
    "### 层归一化LN\n",
    "layer normalization\n",
    "BN是按照样本数计算归一化统计量的，当样本数很少时，样本的均值和方差便不能反映全局的统计分布息，LN是一个独立于batch size的算法，所以无论样本数多少都不会影响参与LN计算的数据量\n",
    "$$\n",
    "\\mu^l = \\frac{1}{H} \\Sigma_{i=1}^H (a_i^l)\\\\\n",
    "\\sigma^l = \\sqrt{\\frac{1}{H} \\Sigma_{i=1}^H (a_i^l - \\mu^l)^2 }\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### 组归一化\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数\n",
    "batch_size = 100\n",
    "original_dim = 1838\n",
    "latent_dim = 64\n",
    "intermediate_dim = 256\n",
    "epochs = 30\n",
    "\n",
    "\n",
    "# =======================================\n",
    "#  构建VAE模型\n",
    "# =======================================\n",
    "batch_norm = True\n",
    "sc_vae, vae_loss, encoder = get_vae_model(batch_norm=batch_norm)\n",
    "sc_vae.add_loss(vae_loss)\n",
    "sc_vae.compile(optimizer='rmsprop')\n",
    "sc_vae.summary()\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 数据加载\n",
    "# =======================================\n",
    "pbmc_data = sc.datasets.pbmc3k_processed()\n",
    "\n",
    "# 读取记录\n",
    "x_data = pbmc_data.X\n",
    "y_test_ = pbmc_data.obs['louvain'].values\n",
    "\n",
    "# 归一化\n",
    "x_train = preprocessing.MinMaxScaler().fit_transform(x_data)\n",
    "\n",
    "# 模型训练\n",
    "sc_vae.fit(x_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_train, None))\n",
    "\n",
    "# 隐含向量\n",
    "\n",
    "\n",
    "x_test_encoded = encoder.predict(\n",
    "    x_train, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 可视化\n",
    "# =======================================\n",
    "features = np.array(x_test_encoded).astype('float32')\n",
    "adata = sc.AnnData(features)\n",
    "adata.obs['label'] = y_test_\n",
    "# 计算邻域图\n",
    "\n",
    "\n",
    "sc.pp.neighbors(adata, n_neighbors=10, use_rep='X')\n",
    "\n",
    "# 聚类\n",
    "sc.tl.louvain(adata)\n",
    "sc.tl.umap(adata)\n",
    "if batch_norm is True:\n",
    "    sc.pl.umap(adata, color=['louvain', 'label'], \n",
    "               save='_pbmc3k_vae_batch_norm_louvain.png')\n",
    "else:\n",
    "    sc.pl.umap(adata, color=['louvain', 'label'], \n",
    "               save='_pbmc3k_vae_layer_norm_louvain.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 变分自编码+HVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自监督学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自监督+批次效应矫正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer + HVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer + Marker分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfromer代码解读\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比较x还是z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rna_seq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
